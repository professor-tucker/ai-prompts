Absolutely. Here's a structured and verbose prompt designed to guide an AI in analyzing and improving the code from Iterations 2 and 3, along with instructions for organizing the code into a "kilo" directory.
Prompt for AI Analysis and Improvement:
Subject: Code Review and Enhancement - AI Assistant MVP (Iterations 2 & 3)
Context:
You are an expert software developer and code reviewer. Your task is to analyze the following code, which represents Iterations 2 and 3 of an MVP for a cross-platform AI assistant. These iterations focus on refactoring, adding type hints, implementing testing, containerization (Docker), and exploring infrastructure as code (Terraform).
Objective:
 * Analyze the Code:
   * Thoroughly review each code file for correctness, efficiency, readability, and adherence to best practices.
   * Identify potential issues, bugs, or areas for improvement.
   * Evaluate the effectiveness of the implemented tests.
   * Assess the Dockerfile and Terraform configurations for completeness and correctness.
 * Provide Feedback and Suggestions:
   * Offer specific, actionable feedback on each code file.
   * Suggest improvements to enhance code quality, performance, and security.
   * Recommend additional tests or modifications to existing tests for better coverage.
   * Propose optimizations for the Dockerfile and Terraform configurations.
 * Generate Improved Code:
   * Based on your analysis and suggestions, generate improved versions of the code files.
   * Ensure the improved code adheres to best practices and addresses identified issues.
   * Maintain the existing functionality while incorporating enhancements.
 * Organize Code for "kilo" Directory:
   * Organize the code into a directory named "kilo" with the following structure:
     * kilo/backend/ (FastAPI backend)
     * kilo/tests/ (pytest tests)
     * kilo/docker/ (Dockerfile and related files)
     * kilo/terraform/ (Terraform configurations)
   * Place each code file within its respective directory.
   * Add file name comments at the top of each file, so it can be parsed correctly.
Code to Analyze:
Iteration 2 Code (Refactoring, Type Hints, and Testing):
# kilo/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, status
from fastapi.security import APIKeyHeader
from fastapi.middleware.cors import CORSMiddleware
import aiohttp
import asyncio
from functools import lru_cache
import os
from dotenv import load_dotenv
from googleapiclient.discovery import build
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
import pickle
import json
import time
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
import csv

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Load environment variables
load_dotenv()

# Initialize FastAPI app
app = FastAPI(title="AI Assistant API", description="Backend API for cross-platform AI assistant")
api_key_header = APIKeyHeader(name="X-API-Key")

# Add CORS middleware (restrict origins in production)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Replace with specific origins in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Gmail API setup
SCOPES = ['https://www.googleapis.com/auth/gmail.readonly', 'https://www.googleapis.com/auth/calendar.readonly']

def get_gmail_credentials() -> Optional[Credentials]:
    """Get and refresh Gmail API credentials."""
    creds: Optional[Credentials] = None
    if os.path.exists('token.pickle'):
        with open('token.pickle', 'rb') as token:
            creds = pickle.load(token)
    
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(
                'credentials.json', SCOPES)
            creds = flow.run_local_server(port=0)
        with open('token.pickle', 'wb') as token:
            pickle.dump(creds, token)
    
    return creds

# Initialize API clients
gmail_service = None  # Will be initialized on first request
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
API_KEY = os.getenv("API_KEY")

@lru_cache(maxsize=128)
async def summarize_email(email_data: str) -> str:
    """Summarize email content using OpenAI API with caching."""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(
                "https://api.openai.com/v1/chat/completions",
                headers={"Authorization": f"Bearer {OPENAI_API_KEY}"},
                json={
                    "model": "gpt-3.5-turbo",
                    "messages": [
                        {"role": "system", "content": "You are a helpful assistant that summarizes emails concisely."},
                        {"role": "user", "content": f"Summarize this email in 2-3 sentences: {email_data}"}
                    ],
                    "max_tokens": 100
                }
            ) as resp:
                if resp.status != 200:
                    logging.error(f"OpenAI API error: {resp.status} - {await resp.text()}")
                    return f"Error: Unable to summarize email (Status {resp.status})"
                
                data = await resp.json()
                return data["choices"][0]["message"]["content"].strip()
    except Exception as e:
        logging.error(f"Error summarizing email: {e}")
        return f"Error summarizing: {str(e)}"

@app.on_event("startup")
async def startup_event():
    """Initialize services on startup."""
    global gmail_service
    try:
        creds = get_gmail_credentials()
        gmail_service = build('gmail', 'v1', credentials=creds)
        logging.info("Gmail service initialized.")
    except Exception as e:
        logging.error(f"Error initializing Gmail service: {e}")

def verify_api_key(api_key: str = Depends(APIKeyHeader(name="X-API-Key"))):
    """Verify API key middleware."""
    if api_key != API_KEY:
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Invalid API Key")
    return api_key

async def fetch_email_data(message_id: str) -> Dict[str, Any]:
    """Fetch email data."""
    if not gmail_service:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Gmail service not initialized")
    
    msg = gmail_service.users().messages().get(userId='me', id=message_id, format='full').execute()
    headers = msg['payload']['headers']
    subject = next((h['value'] for h in headers if h['name'] == 'Subject'), 'No subject')
    sender = next((h['value'] for h in headers if h['name'] == 'From'), 'Unknown sender')
    
    if 'parts' in msg['payload']:
        parts = msg['payload']['parts']
        email_data = ""
        for part in parts:
            if part['mimeType'] == 'text/plain':
                if 'data' in part['body']:
                    import base64
                    email_data += base64.urlsafe_b64decode(part['body']['data']).decode('utf-8')
    else:
        email_data = msg['snippet']
    
    return {'id': message_id, 'subject': subject, 'sender': sender, 'email_data': email_data}

@app.get("/email_digest", dependencies=[Depends(verify_api_key)])
async def get_email_digest(max_emails: int = 5) -> Dict[str, Any]:
    """Fetch and summarize unread emails asynchronously."""
    if not gmail_service:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Gmail service not initialized")

    try:
        results = gmail_service.users().messages().list(userId='me', labelIds=['INBOX'], q="is:unread").execute()
        messages = results.get('messages